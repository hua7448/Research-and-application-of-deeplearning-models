{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanted to see if I could create a neural network that modeled the function below. \n",
    "\n",
    "$ y = x_{0}^2 + \\sin(x_{1}) - x_{2} + 5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(suppress=True) # B/c I hate looking at Numpy's scientific notation when printing matrix values LOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to create the synthetic dataset. Basically we will have two matrices, one that contains all of the examples with the 3 different inputs, and one with all the output values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numExamples = 100\n",
    "numVariables = 3\n",
    "maxValue = 25\n",
    "outputValues = 1\n",
    "\n",
    "allX = np.asarray([np.random.randint(maxValue, size=numVariables) for i in range(numExamples)])\n",
    "allY = np.asarray([x[0]**2 + np.sin(x[1]) - x[2] + 5 for x in allX]).reshape(numExamples, outputValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the shapes of our training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our X matrix shape: (100, 3)\n",
      "Our Y matrix shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "print \"Our X matrix shape:\", allX.shape\n",
    "print \"Our Y matrix shape:\", allY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample input/output pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input: [20  5  7]\n",
      "Expected Y output: [397.04107573]\n"
     ]
    }
   ],
   "source": [
    "print \"X input:\", allX[0]\n",
    "print \"Expected Y output:\", allY[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple one hidden layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numHiddenUnits = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, numVariables])\n",
    "y = tf.placeholder(tf.float32, shape=[None, outputValues])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([numVariables, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.truncated_normal([numHiddenUnits, outputValues], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [outputValues])\n",
    "\n",
    "H1preRelu = tf.matmul(x,W1) + B1\n",
    "H1 = tf.nn.relu(H1preRelu)\n",
    "yLogits = tf.nn.relu(tf.matmul(H1,W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean((yLogits - y)**2)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting problems I came across while doing this is the inability for the network to converge when the initial loss is a certain value. Let's look at what happens below during the first 5 iterations. Run the cell below, and notice the values for the weights in the beginning. They're all centered around 0, and we had a good balance of positive and negative weights. Notice that the loss at the beginning is an extremely large number. This means that the derivative of the loss with respect to the weights is also likely going to be large, which results in large weight updates in the negative direction. \n",
    "\n",
    "Notice how the weight values change from Iteration 0 to 1 to 2 etc. You'll see that almost all of them are negative values (and some of them are really large). Then, take a look at the H1 values after the matmul. A lot of those are negative numbers, and so when we apply our activation function, Relu in this case, we get outputs of all zeros, which means that we'll always get an extremely high loss, and the network won't be able to converge since all the predictions are going to be 0 all the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Iteration 0 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "====================== Iteration 1 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "====================== Iteration 2 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "====================== Iteration 3 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "====================== Iteration 4 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "====================== Iteration 5 ======================\n",
      "Current W1 weights (first dim):\n",
      "[-0.09582855 -0.01781602  0.00541513  0.14701568  0.04391215  0.15258543\n",
      " -0.08114988  0.02420023 -0.0397035  -0.09304301] \n",
      "\n",
      "Current H1 (pre-RELU) values:\n",
      "[-2.283804    1.5308748  -0.9513644   3.6935768   1.5965968   4.8146434\n",
      " -1.4702303  -0.47646996 -0.8460771  -1.2214923 ] \n",
      "\n",
      "Current H1 (post-RELU) values:\n",
      "[0.        1.5308748 0.        3.6935768 1.5965968 4.8146434 0.\n",
      " 0.        0.        0.       ] \n",
      "\n",
      "Current loss: 70307.52 \n",
      "\n",
      "========= Iteration 1000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 2000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 3000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 4000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 5000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 6000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 7000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 8000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n",
      "========= Iteration 9000, Training Loss 70307.5 =========\n",
      "Prediction: 0\n",
      "Label: 397.041\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "trainingIterations = 10000\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    if i <= 5:\n",
    "        _, trainingLoss, curW1, curH1, curH1pre, curPreds = sess.run([opt, loss, W1, H1, H1preRelu, yLogits], feed_dict={x: allX, y: allY})\n",
    "        print \"====================== Iteration %d ======================\"%i\n",
    "        print \"Current W1 weights (first dim):\"\n",
    "        print (curW1[0]),\"\\n\"\n",
    "        print \"Current H1 (pre-RELU) values:\"\n",
    "        print (curH1pre[0]),\"\\n\"\n",
    "        print \"Current H1 (post-RELU) values:\"\n",
    "        print (curH1[0]),\"\\n\"\n",
    "        print \"Current loss:\", trainingLoss,\"\\n\"\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        _, trainingLoss, curPreds = sess.run([opt, loss, yLogits], feed_dict={x: allX, y: allY})\n",
    "        print (\"========= Iteration %d, Training Loss %g =========\" %(i, trainingLoss))\n",
    "        print (\"Prediction: %g\"%(curPreds[0]))\n",
    "        print (\"Label: %g\"%(allY[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens when we divide the loss by some factor in order to decrease the magnitude of some of those gradients and weight updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Iteration 1000, Training Loss 701.674 =========\n",
      "Prediction: 0.157072\n",
      "Label: 397.041\n",
      "========= Iteration 2000, Training Loss 690.644 =========\n",
      "Prediction: 2.44362\n",
      "Label: 397.041\n",
      "========= Iteration 3000, Training Loss 657.483 =========\n",
      "Prediction: 9.50603\n",
      "Label: 397.041\n",
      "========= Iteration 4000, Training Loss 547.657 =========\n",
      "Prediction: 35.5889\n",
      "Label: 397.041\n",
      "========= Iteration 5000, Training Loss 309.314 =========\n",
      "Prediction: 116.434\n",
      "Label: 397.041\n",
      "========= Iteration 6000, Training Loss 203.014 =========\n",
      "Prediction: 242.384\n",
      "Label: 397.041\n",
      "========= Iteration 7000, Training Loss 165.778 =========\n",
      "Prediction: 211.31\n",
      "Label: 397.041\n",
      "========= Iteration 8000, Training Loss 128.334 =========\n",
      "Prediction: 298.193\n",
      "Label: 397.041\n",
      "========= Iteration 9000, Training Loss 102.626 =========\n",
      "Prediction: 259.967\n",
      "Label: 397.041\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(((yLogits - y)**2)/100) ###### LINE WITH THE CHANGE ######\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .01).minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "trainingIterations = 10000\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "#    if i <= 5:\n",
    "#        _, trainingLoss, curW1, curH1, curH1pre, curPreds = sess.run([opt, loss, W1, H1, H1preRelu, yLogits], feed_dict={x: allX, y: allY})\n",
    "#        print \"====================== Iteration %d ======================\"%i\n",
    "#        print \"Current W1 weights (first dim):\"\n",
    "#        print (curW1[0]),\"\\n\"\n",
    "#        print \"Current H1 (pre-RELU) values:\"\n",
    "#        print (curH1pre[0]),\"\\n\"\n",
    "#        print \"Current H1 (post-RELU) values:\"\n",
    "#        print (curH1[0]),\"\\n\"\n",
    "#        print \"Current loss:\", trainingLoss,\"\\n\"\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        _, trainingLoss, curPreds = sess.run([opt, loss, yLogits], feed_dict={x: allX, y: allY})\n",
    "        print (\"========= Iteration %d, Training Loss %g =========\" %(i, trainingLoss))\n",
    "        print (\"Prediction: %g\"%(curPreds[0]))\n",
    "        print (\"Label: %g\"%(allY[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like you can see above, the network is definitely training and getting closer to the outpt value. So, the value of your initial loss seems to be very important to whether or not your network will converge. This is a toy example, but it is useful to keep in mind whenever you  The two fixes that immediately come to mind are:\n",
    "\n",
    "1) Decreasing the learning rate so you get smaller weight updates.\n",
    "\n",
    "2) Dividing loss function by some constant to make optimization easier. \n",
    "\n",
    "Anything else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
